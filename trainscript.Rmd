---
title: "A Machine Learning Model in Two (and a half) Steps"
author: "Marc Emanuel"
date: "03/15/2015"
output: html_document
---
## Introduction

The goal  of this machine learning script is to train the data made gratiously availlable by [groupware@les](http://groupware.les.inf.puc-rio.br/har) in order to predict the outcome of a qualification in 5 clases of an excercise with dumbbells as performed by 6 individuals. The predictors consist mostly of the signas of several sensors attached to the person performing the excercises. Since the number of predictors is reasonable big and we have a classification problem, the choice goes out to a a tree based scheme. An alternative would be a Bayesian neural network, like the algorithm that won the [2003 NIPS price](http://clopinet.com/isabelle/Projects/NIPS2003/analysis.html), but not having enough expertise to carry that out within the limited available time, I resorted to trusted water.  

The plan is the following: after splitting off ine quarter of the data for testing the final model, I use a classification tree with stochastic boosting (the gbm package) through the caret interface with all features that are left after a first selection based on simple observations. 
The resulting relative influence (the relative decrease in the squared error attributed to a predictor) is used as a guide to reduce the set of predictors to a reasonable (in terms of time) featureset size of $42$.  

After this feature set reduction random forest is used. This is also done using the caret package which runs through the parameterset (in this case just one parameter the number of choices per node),using a cross-validation of choice. The default of a $10$-fold cross-validation with a bootstrapped cross validation set, resulted in an accuray and a $\kappa$ (taking sucess caused by random luck into account)of over $98\%$, which seems to be more than enough.

Then the quality of the model is tested using the split off testing set

Finally the model is tested using the coursera provided test data and it completes this without any error.  And then for something completely different....
 
## Preprocessing the raw data
```{r cache=TRUE}

library(caret)
library(doMC) # making use of available cores
registerDoMC(cores = 3)# change this to the number of cores you want to use
na <- c('NA','') # added the empty strings to elinate the time window averages
data <- read.csv(file="pml-training.csv", na.strings=na)
sel <- createDataPartition(data$classe,p=3/4,list = FALSE)
training <- data[sel,]
testing <- data[-sel,]
```
There are a whole bunch of features that only contain averages ove a time period, They will contain NA's 
Furthermore we get rid of the index column the timestamps, and the time window number, which will play a montyesque role in the end. 

```{r cache=TRUE, dependson=-1}
logic <- apply(training, 2, function(x) sum(is.na(x))==0)
logic[1:7]<-FALSE # the time related features
training<-training[,logic]
```
We do first a rough classification to be able to filter the features based on their influence using gbm

```{r cache=TRUE, dependson=c(-1,-2)}
registerDoMC(cores = 3)
set.seed(1999) # Prince !
fit=train(classe~.,data=training, method='gbm',trControl = trainControl(verbose = FALSE))
```
The summary plot shows a barplot of the ordered influences per feature. I stick with the original plan of picking all nonzero influence predictors. 

```{r, cache=TRUE,dependson=c(-1)}
summ<-summary(fit)
print(summ,row.names=FALSE)# pretty print
```

```{r, cache=TRUE, dependson=c(-1,-2)}
keep<-c(as.character(summ$var[1:42]), "classe")
training<-training[keep]
```

## Training
That is all the preprocessing needed. Note that most other forms of preprocessing (scaling centering, PCA) are pointless here and 42 is a symbolic number after all. There is also no fear of overfitting with randomForest. What is more there is no need to use a separate cross-validation step when searching for model parameters. Caret takes care of that it is beautiful! 

```{r, cache=TRUE, dependson=c(-1)}
registerDoMC(cores = 3)
set.seed(1984)  # Orwell!
# and there we go, time for coffee
fit2<-train(classe~.,data=training,method='rf')
# a little later we summarize the test results
print(fit2)
plot(fit2)
```

The accuracy of 98.7 % is reasonable nut perhaps we can do better by using folding instead of bootstrapping.
  
```{r, cache=TRUE, dependson=c(-1)}
registerDoMC(cores = 3)
set.seed(1984)  # Orwell!
seeds <- vector(mode = "list", length = 11)
for(i in 1:10) seeds[[i]] <- sample.int(1000, 22)
## For the last model:
seeds[[11]] <- sample.int(1000, 1)
ctrl <- trainControl(method = "cv", 
                     seeds = seeds)
# and there we go, time for coffee
fit3<-train(classe~.,data=training,method='rf',trControl=ctrl,importance = TRUE)
# a little later we summarize the test results
print(fit3)
summary(fit3)
plot(fit3)
```
Note the accuracy, and $\kappa$ of $99\%$! And these are all sort of out of sample errors. They have virtually no bias. Of course it does not suffice since we used all of the training-set data for tuning, thus we need the test sample that has been untouched.

The plot shows that a number of predictors of $2$ per node optimal is. The standard number of predictors for randomForest is the square root of the number of features or $6-7$ predictors. Tuning was marginally useful ($0.2\%$ accuracy increase)

Now we can test, I decided to test all 3 training models , although no conclusion can be drawn from this !!! Otherwise we would bias our result in favor of the testing set. Anyway the testing accuracy's are fine

## Testing

```{r}
library(caret)
pred1<-predict(fit,testing)
pred2<-predict(fit2,testing)
pred3<-predict(fit3,testing)
confusionMatrix(pred1,testing$classe)
confusionMatrix(pred2,testing$classe)
confusionMatrix(pred3,testing$classe)
```







```{r, eval = FALSE,cache=TRUE, dependson=c(-2,-3,-4)}
coursetest<-read.csv("pml-testing.csv", na.strings=na)
# no need to process since we didn't transform
pred<-as.character(predict(fit3,coursetest))
# the function as provided, why not just in one file, would have been more efficient
pml_write_files = function(x){
   n = length(x)
   for(i in 1:n){
     filename = paste0("problem_id_",i,".txt")
     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
   }
}
pml_write_files(pred)
```
That is a $100\%$ accuracy on the test set,though the test set is rather small, so some luck plays also a role. Wanting to get rid of any luck factor Let us have a look at the dataset a bit more carefully 

## Surprise
Notice the time window number. The time windows are pretty damn short. It looks as if the the quaity of the movement can never change during such a time frame. A fast check shows this indeed to be the case. What is more, the coursera test data has time intervals that are also in the training set, so we could have gotten the right answers not by training but using a simple lookup method!!!

This is also the reason people reported to have used a low number of features, they inknowingly had the cheating data in their feature selection.
Now this is in fact close to cheating because the time windows are not usuable for unknow trainings, but we were explicitly allowed to use any of the variables. Clearly this learning algorithm will have to fail if the test set and training set would have no overlap in time windows. I rest my case.